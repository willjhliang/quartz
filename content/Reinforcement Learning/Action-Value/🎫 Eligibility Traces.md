Eligibility traces bridges function approximation [[ü™ô Monte Carlo Control]] and [[‚åõÔ∏è Temporal Difference Learning]] by averaging [[ü™ú N-Step Bootstrapping]]. For $n$-step return $$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}),$$ the $\lambda$-return is defined as $$G^\lambda_t = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n}.$$ This is a exponentially-weighted average with $(1-\lambda)$ in front to normalize the sum. In the finite horizon, we let post-termination returns be the conventional return $G_t$, so $$G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t:t+n} + \lambda^{T-t-1}G_t.$$

Note that with $\lambda = 0$, we get the one-step TD return, and with $\lambda=1$, we have a Monte Carlo estimate.

However, this formulation isn't commonly used because it requires complex computations for the $n$-step returns. We can instead use an eligibility trace $z_t$ that has the same dimension as our weights $\theta_t$. This vector represents the magnitude of the update performed to $\theta_t$ at each time step, and it can viewed as a "backward" way of updating the weights rather than the "forward" return method above.

Specifically, we initialize $z = 0$, and at each transition, $$z_t = \gamma \lambda z_{t-1} + \nabla_\theta V_\theta(S_t).$$ The eligibility trace is then used to update our weights, $$\theta_{t+1} = \theta_t + \alpha E z_t$$ where $E$ is some error measure. From this formulation, we can see that the eligibility trace essentially captures the "recency" of updates to our weights; similar to $\lambda$-return, it places more emphasis on the most recent values.

# TD($\lambda$)
TD($\lambda$) is the most common implementation of eligibility traces. Our error $E$ is set as the TD error $$\delta_t = R_{t+1} + \gamma V_\theta(S_{t+1}) - V_\theta(S_t),$$ and our weight update can be performed at every time step, similar to standard [[‚åõÔ∏è Temporal Difference Learning#TD(0)]]. Observe that again, with $\lambda = 0$, we (quite literally) get TD(0), and with $\lambda = 1$, we get TD(1), a Monte Carlo variant that discounts rewards.