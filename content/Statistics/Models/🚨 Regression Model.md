A regression model uses a distribution to model some function from predictors $X$ to $y$. Here, we'll consider the classical linear regression model, where we assume the data is generated by 
$$
y_i = X_i \beta + e_i
$$
 where noise $e_i \sim \text{Normal}(0, \sigma^2)$.

Our model has the form 
$$
y_i \sim \text{Normal}(X_i \beta, \sigma^2),
$$
 which looks very much like the [[ðŸ›Žï¸ Normal Model]] except with the mean being determined by $X_i = (x_{i1}, \ldots x_{ip})$. Within these $p$ predictor variables $x_{ij}$, we assume there's an intercept (constant at $1$).

# Non-Informative Prior
Our normal model has the improper non-informative prior $p(\mu) \propto 1$ and $p(\sigma^2) \propto 1/\sigma^2$, so for our normal regression model, we'll use a similar $p(\beta) \propto 1$ and $p(\sigma^2) \propto 1/\sigma^2$.

The joint posterior is 
$$
p(\beta, \sigma^2 \vert y) \propto (\sigma^2)^{-(n+2)/2} \exp \left\{ -\frac{1}{2\sigma^2} (y-X\beta)^\top (y-X\beta) \right\}.
$$
 Again breaking this up, we have the conditional posterior 
$$
\beta \vert \sigma^2, y \sim \text{MVNormal}(\hat{\beta}, \sigma^2 \cdot V_\beta)
$$
 where $\text{MVNormal}$ stands for multivariate normal, $\hat{\beta} = (X^\top X)^{-1} X^\top y$ and $V_\beta = (X^\top X)^{-1}$.

The marginal posterior is 
$$
\sigma^2 \vert y \sim \text{InvGamma}\left( \frac{n-p}{2}, \frac{\sum_i (y_i - X_i \hat{\beta})^2}{2} \right).
$$


## Sampling
To sample estimates for $\beta$ and $\sigma^2$ as well as new $y^*$, we perform the following:
1. Sample $x \sim \text{Gamma}((n-p)/2, \ldots)$.
2. Set $\sigma^2 = 1/x$.
3. Sample $\beta \vert \sigma^2, y \sim \text{MVNormal}(\ldots)$.
4. Sample $y^* \vert \beta, \sigma^2 \sim \text{Normal}(X^* \beta, \sigma^2)$.

# Conjugate Prior
We'll keep the non-informative prior for $\sigma^2$, but we'll now use a conjugate prior for $\beta$, 
$$
\beta_k \sim \text{Normal}(\gamma_k, \tau^2).
$$


The posterior is 
$$
p(\beta, \sigma^2 \vert y) \propto (\sigma^2)^{-(n+2)/2} \exp \left\{ -\frac{1}{2\sigma^2} (y-X\beta)^\top (y-X\beta) - \frac{1}{2\tau^2} \sum_k (\beta_k - \gamma_k)^2\right\}.
$$


The conditional posterior is 
$$
\beta \vert \sigma^2, y \sim \text{MVNormal}(\hat{\beta}, V_\beta)
$$
 where $\hat{\beta} = ({X^*}^\top\Sigma^{-1}X^*)^{-1}{X^*}^\top\Sigma^{-1}y^*$ and $V_\beta = ({X^*}^\top \Sigma^{-1}X^*)^{-1}$, 
$$
y^* = \begin{pmatrix} y \\ \gamma \end{pmatrix},\ X^* = \begin{pmatrix} X \\ I_p \end{pmatrix},\ \Sigma = \begin{pmatrix} \sigma^2 I_n & 0 \\ 0 & \tau^2 I_p \end{pmatrix},
$$
 and the marginal posterior for $\sigma^2$ is the same as above except with our new definition for $\hat{\beta}$.

## Connection to Machine Learning
We often want sparsity in our modelâ€”for as many coefficients $\beta_k$ to be zero as possible, thereby giving us a subset of *important* predictors with non-zero coefficients. We can encode this belief into our conjugate prior as 
$$
\beta_k \sim \text{Normal}(0, \tau^2),
$$
 which pulls each $\hat{\beta}_k$ toward zero.

Without this prior (using the non-informative prior), we get the least square estimate. With the prior, we have a penalty term $p(\beta)$ in our minimization objective.
1. With the sparsity Normal prior above, the penalty becomes $\lambda \sum_k \beta_k^2$ which is called ridge regression.
2. With a Laplace prior $p(\beta_k) \propto e^{-\lambda \vert \beta_k \vert}$, we have the penalty $\lambda \sum_k \vert \beta_k \vert$, called LASSO regression.

See [[ðŸ¦ Linear Regression]] and [[âš½ï¸ Regularization Penalties]] for more details.

# Poisson Model
Regression can also be done with the [[ðŸ›©ï¸ Poisson Model]]. We have 
$$
y_t \sim \text{Poisson}(\alpha + \beta t)
$$
 where $\alpha + \beta t > 0$.

Using a non-informative joint prior $p(\alpha, \beta) \propto 1$, we have the posterior 
$$
p(\alpha, \beta \vert y) \propto \prod_t (\alpha + \beta t)^{y_t} e^{-(\alpha + \beta t)}.
$$
 We can use [[ðŸ§± Grid Sampling]] to get parameter samples $\alpha^*, \beta^*$, which we can then use for prediction.