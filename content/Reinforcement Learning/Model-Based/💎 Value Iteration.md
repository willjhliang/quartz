Value iteration is a reinforcement learning algorithm that estimates the Q-function and value function using the environment's dynamics. The algorithm performs the following two steps for every state $s$:
1. 
$$
Q(s, a) \leftarrow r(s, a) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, a)}[V(s')].
$$

2. 
$$
V(s) \leftarrow \max_a Q(s, a).
$$


An alternative formula merges the two steps, repeating a single instruction instead: 
$$
V(s) \leftarrow \max_a \mathbb{E}_{(s', r) \sim p(s' \vert s, a)} [r(s, a) + \gamma V(s')]
$$


This method implicitly finds a deterministic optimal policy since the second step is assuming that our policy takes the best action following $Q$, 
$$
\pi(a \vert s) = \arg\max_a Q(s, a).
$$
 

We can interpret value iteration as a simpler version of [[‚ôªÔ∏è Policy Iteration]] where we merge the policy improvement step into policy evaluation. It also follows the same convergence guarantee for the tabular case; the intuition is that our algorithm continuously contracts the distance between our current $V$ and the optimal $V^*$, which ensures convergence.

# Fitted Value Iteration
While the above assumes the $V(s)$ is a table with discrete states $s$, it's often not possible to use this tabular method when the state space is huge. Fitted value iteration replaces the table with a neural network $V_\phi(s)$, and our algorithm must learn to fit the network in the second step:
1. Set 
$$
y_i \leftarrow \max_{a_i} \{ r(s_i, a_i) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, a)}[V_\phi(s')] \}.
$$

2. Set 
$$
\phi \leftarrow \arg\min_\phi \frac{1}{2} \sum_{i=1}^N \Vert V_\phi(s_i) - y_i \Vert^2.
$$


Unfortunately, when we introduce a neural network, our convergence guarantee doesn't work. Our neural network effectively restricts the space of possible $V$ functions, and if we contract toward $V^*$ and then fit our neural network on the desired update, it's possible to move further from the optimal parameters.

# Fitted Q-Iteration
Value iteration and fitted value iteration both require knowing the transition probability $p(s' \vert s, a)$. We can circumvent this by fitting the $Q_\phi$ function instead as follows:
1. Set 
$$
y_i \leftarrow \max_{a_i} \{ r(s_i, a_i) + \gamma \max_{a'} Q_\phi(s_i', a_i') \}.
$$

2. Set 
$$
\phi \leftarrow \arg\min_\phi \frac{1}{2} \sum_{i=1}^N \Vert Q_\phi(s_i, a_i) - y_i \Vert^2.
$$


Note that in the first step, we approximate 
$$
\mathbb{E}_{s' \sim p(s' \vert s, a)}[V_\phi(s')] \approx \max_{a'} Q_\phi(s_i', a_i')
$$
 using a single sample $s_i'$, and our value function is defined by the best action like above. Fitted Q-iteration now only requires $(s_i, a_i, s_i', r_i)$ tuples which can be generated by any policy, and thus it relaxes our reliance on $p(s' \vert s, a)$. We can collect these tuples in a dataset before training or perform online updates; the latter gives us the function approximation version of [[üöÄ Q-Learning]].