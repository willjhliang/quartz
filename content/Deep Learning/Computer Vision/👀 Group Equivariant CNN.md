Group Equivariant CNNs (G-CNNs) generalized the standard [[üëÅÔ∏è Convolutional Neural Network]]s' translation [[ü™û Equivariance]] to other symmetries, specifically rotation and reflection.

# Symmetric Groups
A symmetry of an object is a transformation that leaves it invariant. The set of symmetric transformations is a symmetry group.

The translation, rotation, and reflection operations are all symmetries for our feature map. Thus, they form the p4m group parameterized by $$g(m, r, u, v) = \begin{bmatrix} (-1)^m \cos(r\pi/2) & -(-1)^m\sin(r\pi/2) & u \\ \sin(r\pi/2) & \cos(r\pi/2) & v \\ 0 & 0 & 1 \end{bmatrix}.$$ This matrix is applied to [[üìΩÔ∏è Projective Geometry#Homogenous Coordinates]] for the transformed pixels. That is, a member of the group $g$ can be applied to a pixel location via $gx$.

# Group Functions
However, observe a transformation on a pixel is equivalent to the inverse transformation on the image; for example, moving a pixel to the left in $\mathbb{R}^2$ space is equivalent to shifting the image to the right in the same pixel space. Thus, the transformation acting on a feature map $f$ is defined as $$[L_gf](x) = f(g^{-1}x).$$ Intuitively, this is saying that the value at point $x$ after transforming $f$ via $L_g$ comes from the point $g^{-1}x$ in the original feature map.

A feature map in p4m space and the operations a group can perform on it is below.

![[20230310184114.png#invert|400]]

# G-Convolutions
Now, we can generalize standard convolutions to groups. Note that the original convolution is CNNs is performed on a group consisting of translations; for a filter $\psi$ and feature map $f$, this convolution is defined as $$[f * \psi](x) = \sum_{y \in \mathbb{Z}^2} f(y)\psi(y-x)$$ for each offset $x$. That is, the convolution computes the product across the feature map and a translated (offset) filter.

The key insight here is that $x \in \mathbb{Z}^2$ is equivalent to the group parameterized by $$g(u, v) = \begin{bmatrix} 1 & 0 & u \\ 0 & 1 & v \\ 0 & 0 & 1 \end{bmatrix}.$$ Thus, instead of thinking about $x$ as simply a tuple, we can imagine it as a member of this translation group, meaning that each output of the convolution is the result of a specific parameterization of $u$ and $v$ in $g$. Fundamentally, the convolution is a function that takes groups as input rather than a simple offset.

Therefore, we can generalize to any symmetric group $G$: for an operation $g \in G$, we can write the above equation as $$[f * \psi](g) = \sum_{y \in \mathbb{Z}^2} f(y)\psi(g^{-1}y).$$ This is the G-convolution that acts on the input feature map.

However, since this results in a feature map $f * \psi$ that's a function on $G$, all other G-convolutions require filters that are functions on $G$, giving us $$[f * \psi](g) = \sum_{h \in G} f(h)\psi(g^{-1}h).$$

Pooling layers can be defined using the same idea, and nonlinearities are still performed over each value, $\mathbb{R} \rightarrow \mathbb{R}$. These three building blocks thus allow us to build a CNN that has equivariance for translations, rotations, and reflections; note that in implementation, only 90-degree rotations were implemented.