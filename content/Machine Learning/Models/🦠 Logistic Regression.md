# Theory
Logistic regression uses a similar idea as [[ðŸ¦ Linear Regression]] but transforms the output to probabilities in range $(0, 1)$ using the sigmoid function $f(x) = \frac{1}{1+e^{-x}}$ (pictured below).

![[20221229103241.png#invert|300]]

Weâ€™ll apply weights $\theta$ to input $x$, then run it through the sigmoid function to get a probability for each label. In the binary case, if we let labels be $\{1, -1\}$, then probability is as follows.

$$
 P(y \vert x; \theta) = \frac{1}{1 + \exp\{-y\theta^\top x\}} 
$$

The equation above enforces linear decision boundary at $\theta^\top x = 0$, where probabilities are equal. Furthermore, we find that the log odds 
$$
\log\left(\frac{P(Y=1 \vert X)}{P(Y=-1 \vert X)}\right) = \theta^\top x
$$

is linear on $x$. The decision boundary makes more sense as it's equal to $\log(1)$, 

Now, the likelihood of our data can be calculated as a product, 
$$
L(\theta) = \prod_{i=1}^n \frac{1}{1+\exp\{-y^{(i)}\theta^\top x^{(i)}\}}
$$

Itâ€™s easier to optimize the log likelihood $\ell(\theta)$, which simplifies to the following.

$$
 \ell(\theta) = -\sum_{i=1}^n \log(1 + \exp\{-y^{(i)}\theta^\top x^{(i)}\}) 
$$

With simple logistic regression, this is a concave down function with global optimum, which can be optimized via gradient ascent; this is also equivalent to minimizing the negation, which can be seen as optimizing logistic loss.

> [!note]
> We can also view our loss as a [[ðŸ’§ Cross Entropy]] loss, between the true one-hot encoded labels and probabilities generated by our model.

We can generalize logistic regression to softmax regression, which classifies multiple classes. The probability of each class is the ratio of its probability with respect to all classes, mathematically computed as 
$$
P(y = k \vert X, \theta) = \frac{\exp\{\theta_k^\top x\}}{\sum_{k'=1}^K\exp\{\theta_{k'}^\top x\}}
$$


# Model
Like linear regression, our model consists of weights $\theta$; the main difference is that we use the sigmoid function after apply the weights to get a probability.

For multi-class classification, weâ€™ll calculate individual probabilities for each class. With $K$ classes, use $K$ sets of weights $\theta_1, \ldots, \theta_k$.

# Training
Given training data $x$ and labels $y$, assume weight prior $w_j \sim \mathcal{N}(0, \lambda^2)$. Since thereâ€™s no closed form solution, weâ€™ll employ gradient ascent.
1. Randomly initialize weights $w$.
2. Repeatedly perform gradient ascent steps; gradient is calculated as 
$$
\nabla_\theta \ell (\theta) = \sum_{i=1}^n y^{(i)}x^{(i)}(1 - P(y^{(i)} \vert x^{(i)}; \theta)) - \frac{1}{\lambda^2}\theta
$$
 Note that for MLE, we drop the regularization term $-\frac{1}{\lambda^2}\theta$. The actual gradient ascent update is 
$$
\theta^{t+1} = \theta^\top + \eta_t \nabla_\theta \ell(\theta)
$$


# Prediction
Given input $x$, calculate 
$$
P(y = 1 \vert x; \theta) = \frac{1}{1 + \exp\{-\theta^\top x\}}
$$

If itâ€™s above threshold $0.5$, then classify as $y = 1$; otherwise, classify as $y = 0$. For multi-class, return the class that had highest probability.