Models with residual connections generally have the form $$h_{t+1} = h_t + f(h_t, \theta_t)$$ with hidden layers $h$. We can view this as a discretized transformation of $h$; in other words, we're changing $h$ at discrete time steps $t = 1, \ldots, k$ with function $f$.

Neural ODEs present a continuous version of this transformation, $$\frac{dh(t)}{dt} = f(h(t), t, \theta)$$ where $t$ is now continuous, and $f$ is now a trainable model with parameters $\theta$. This is an implicit expression of our solution $h(t)$, which we can solve for using any ODE solver.

A better interpretation of $h$ is not as hidden layers but instead as a hidden variable $z$ in a dynamical system. If we let our start time be $t_0$ and end time be $t_1$, then $z(t_1)$, defined by the ordinary differential equation above, is our network's prediction. Our goal is thus to minimize the loss $$L(z(t_1)) = L\left( z(t_0) + \int_{t_0}^{t_1} f(z(t), t, \theta)dt \right) = L(\text{ODESolve}(z(t_0), f, t_0, t_1, \theta)).$$

However, computing the gradient $\frac{\partial L}{\partial \theta}$ isn't as simple as other networks since our loss is now defined by a solution to the ODE, and $\theta$ are the parameters to $f$, not $z$. Instead, we need to first relate the loss $L$ with our states $z(t)$; thus, we first introduce the adjoint $$a(t) = \frac{\partial L}{\partial z(t)},$$ which follows dynamics defined by $$\frac{da(t)}{dt} = -a(t)^\top \frac{\partial f(z(t), t, \theta)}{\partial z}.$$

![[20230302202647.png#invert|400]]

We have the value for $a(t_1) = \frac{\partial L}{\partial z(t_1)}$, so we can find $a(t_0)$ via another call to the ODE solver going backwards from $t_1$ to $t_0$. Finally, to compute the gradient update, we have $$\frac{dL}{d\theta} = -\int_{t_1}^{t_0} a(t)^\top \frac{\partial f(z(t), t, \theta)}{\partial \theta} dt.$$

# Continuous Normalizing Flows
Another common occurrence of the residual connection formula is in [[ðŸ’¦ Normalizing Flow]]s with the change of variables formula $$\log p(z_1) = \log p(z_0) - \log \left\vert \det \frac{\partial f}{\partial z_0} \right\vert \det$$ where $z_1 = f(z_0)$. Applying the same continuous idea to this transformation, we get $$\frac{\partial \log p(z(t))}{\partial t} = -\text{tr}\left( \frac{df}{dz(t)} \right).$$ This simplifies the normalizing constant to require only a linear computation rather than expensive Jacobian in standard normalizing flows. Experiments have shown that this continuous model is competitive with the standard method.