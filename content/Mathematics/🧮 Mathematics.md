Mathematics is the core of many algorithms and intuitive ideas. The following are the main topics used in computer science with a slight bias toward machine learning fundamentals.

# Linear Algebra
Linear algebra is the study of vectors and matrices and their manipulations.
1. [[ğŸ¹ Vector]]s are objects that are closed under clearly defined addition and scalar multiplication operations.
2. A [[ğŸ± Matrix]] is a two-dimensional tuple with entries; important properties of a matrix include the [[ğŸ“– Determinant]] and [[ğŸ–Šï¸ Trace]] as well as its [[ğŸ’ Eigenvalue]]s.

Using vectors and matrices, we can solve [[âš™ï¸ System of Linear Equations]] and model [[ğŸ—ºï¸ Linear Mapping]]s.

Sometimes, a problem requires a matrix to be factorized.
1. [[ğŸ¥§ Cholesky Decomposition]] converts a PSD matrix into two triangular matrices.
2. [[ğŸª· Eigendecomposition]] shows that a non-defective square matrix is similar to a diagonal matrix.
3. [[ğŸ“ Singular Value Decomposition]] decomposes a matrix into the product of two orthonormal matrices and one diagonal matrix with singular values.

# Geometry
Geometry is closely tied with vectors and matrices from linear algebra, and we can use it to interpret them from a new point of view.
1. [[ğŸ³ Inner Product]]s map pairs of vectors to a number, and [[ğŸ“Œ Norm]]s represent the size of vectors and matrices.
2. With them, we can compute [[ğŸš— Distance]]s, [[ğŸ“ Angle]]s between vectors, and [[ğŸ“½ï¸ Projection]]s of vectors onto subspaces.
3. [[ğŸª© Rotation]]s can also be defined by matrices as linear mappings.

# Calculus
Calculus describes the shape of functions in detail and is crucial for optimization and approximations.
1. [[ğŸ§ Derivative]]s find the tangent slope in univariate functions, and [[â„ï¸ Gradient]]s generalize them to multivariate functions.
2. The [[ğŸ¤ Taylor Series]] is an important method for approximating any differentiable function as a polynomial.

# Optimization
Using calculus, we can solve general minimization problems.
1. [[ğŸ‘Ÿ Unconstrained Optimization]] uses gradient descent to walk down convex objectives.
2. [[ğŸ‘  Constrained Optimization]] applies the augmented Lagrangian to represent a problem in dual form.

# Probability Theory
Probability theory measures the likelihood of events and outcomes.

[[ğŸª Random Variable]]s measure some quantitative value over [[ğŸ² Probability Distribution]]s.
1. [[ğŸ‡ºğŸ‡¸ Independence]] between two variables is an important property that leads to further conclusions.
2. [[ğŸ“š Summary Statistics]] explains important properties of a random variable's distribution.

Among all classes of probability distributions, the [[ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Exponential Family]] is commonly used due to its simplicity and computational properties. The most common member is the [[ğŸ‘‘ Gaussian]].

Generalizing to any distribution, there are several crucial theorems and conclusions.
1. [[ğŸª™ Bayes' Theorem]] relates the posterior, prior, likelihood, and evidence.
2. [[ğŸŒˆ Jensen's Inequality]] measures the effect of applying a convex function to a random variable.
3. [[ğŸ§¬ Evidence Lower Bound]] provides a tractable lower bound on an intractable likelihood.

Lastly, information theory is a subfield that analyzes the entropy of distributions.
1. [[ğŸ”¥ Entropy]] measures the level of uncertainty in a distribution.
2. [[ğŸ’§ Cross Entropy]] generalizes entropy to compute across two distributions.
3. [[ğŸ’° Information Gain]] measures the change in entropy after gaining new "information."
4. [[ğŸ¤ Mutual Information]] measures the degree of shared "information" between two variables.
5. [[âœ‚ï¸ KL Divergence]] and, more generally, [[ğŸª­ F-Divergence]] measure the difference between two distributions.