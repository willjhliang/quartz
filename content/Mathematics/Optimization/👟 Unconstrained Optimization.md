# Problem
Given a convex objective $F(w)$ and for parameters $w \in \mathcal{L}$, a convex set, we want to find $$\min_w F(w) \text{ such that } w \in \mathcal{L}.$$
## Convexity
A convex set is defined as a set where for all $w, w' \in \mathcal{L}$ and $\alpha \in [0, 1]$, $$\alpha w + (1 - \alpha)w' \in \mathcal{L}.$$ Intuitively, this is saying that the line between $w$ and $w'$ is inside the set.

A convex function is defined as $F$ such that for $w, w' \in \mathbb{R}^d$ and $\alpha \in [0, 1]$, $$F(\alpha w + (1 - \alpha)w') \leq \alpha F(w) + (1 - \alpha)F(w').$$ In other words, the function is upwards curving. This is also known as [[üåà Jensen's Inequality]].

Moreover, if $F$ is convex and differentiable, then for all $w, w'$, $$F(w') \geq F(w) + \nabla F(w)^\top (w' - w).$$ This is illustrated below.

![[20230217121603.png#invert|500]]

Lastly, for all $w$, $\nabla^2 F(w)$ is a positive semidefinite matrix. This is also saying that the function always curves upwards.

# Solution
If $F$ is convex and differentiable, any $w$ that satisfies $\nabla F(w) = 0$ is a global minimum.

## Convex Properties
A convex function $F$ is $L$-smooth if $$F(w') \leq F(w) + \nabla F(w)^\top (w' - w) + \frac{L}{2}\Vert w - w' \Vert_2^2.$$

A function $F$ is $\mu$-strongly convex if $$F(w') \geq F(w) + \nabla F(w)^\top (w' - w) + \frac{\mu}{2} \Vert w - w' \Vert_2^2.$$

## Gradient Descent
These two properties essentially bound our function with $L$ and $\mu$. We'll use them to inform our algorithm below, called [[‚õ∞Ô∏è Gradient Descent]].

1. Initialize $w_1 \in \mathbb{R}^d$.
2. For $t$ time steps,
	1. Update $w_{t+1} = w_t - \eta_t\nabla F(w_t)$.

For a $L$-smooth convex function $F$, since we have an upper bound, we can minimize the quadratic $$\min_{w'} F(w) + \nabla F(w)^\top (w' - w) + \frac{L}{2}\Vert w' - w \Vert_2^2,$$ which gives us $w' = w - \frac{1}{L} \nabla F(w)$, our gradient descent step.

With this update rule setting $\eta = \frac{1}{L}$, we will converge to optimal $w_*$. Specifically, at time $t$, we have $$F(w_{t+1}) - F(w_*) \leq \frac{L}{2t} \Vert w_1 - w_* \Vert_2^2.$$ Also, if we let our acceptable error $\epsilon = F(w_{t+1}) - F(w_*)$, we converge in $O(\frac{1}{\epsilon})$ steps.

---
**Proof of Convergence**

We'll start with the smoothness property, $$F(w_{t+1}) \leq F(w_t) + \nabla F(w_t)^\top (w_{t+1} - w_t) + \frac{L}{2}\Vert w_{t+1} - w_t \Vert^2.$$

By our gradient descent step, we know $w_{t+1} = w_t - \frac{1}{L}\nabla F(w_t)$, so rearranging for $\nabla F(w_t) = L(w_t - w_{t+1})$ and plugging it into our equation above, we get $$F(w_{t+1}) \leq F(w_t) + L(w_t - w_{t+1})^\top (w_{t+1} - w_t) + \frac{L}{2} \Vert w_{t+1} - w_t \Vert^2$$ and simplifying, we have $$F(w_{t+1}) - F(w_t) \leq -\frac{L}{2} \Vert w_{t+1} - w_t \Vert^2.$$

Next, we use the convex property to get $$F(w_*) \geq F(w_t) + \nabla F(w_t)^\top (w_* - w_t),$$ and rearranging and plugging in the gradient update above, $$F(w_t) - F(w_*) \leq L(w_t - w_{t+1})^\top (w_t - w_*).$$ Then, since $$2a^\top b = \Vert a \Vert^2 + \Vert b \Vert^2 - \Vert a - b \Vert^2,$$ we let $a = (w_t - w_{t+1})$ and $b = (w_t - w_*)$ and replace the above to get $$F(w_t) - F(w_*) \leq \frac{L}{2}(\Vert w_{t+1} - w_t \Vert^2 + \Vert w_t - w_* \Vert^2 - \Vert w_* - w_{t+1} \Vert^2).$$ Adding the inequality we derived above, we get $$F(w_{t+1}) - F(w_*) \leq \frac{L}{2}(\Vert w_t - w_* \Vert^2 - \Vert w_* - w_{t+1} \Vert^2)$$
Finally, summing $t$ on both sides, we have the following: $$\begin{align*}\sum_{t=1}^\top [F(w_{t+1}) - F(w_*)] &\leq \sum_{t=1}^\top \frac{L}{2}(\Vert w_t - w_* \Vert^2 - \Vert w_* - w_{t+1} \Vert^2) \\ T[F(w_{T+1}) - F(w_*)] & \leq \frac{L}{2}(\Vert w_1 - w_* \Vert^2 - \Vert w_* - w_{T+1} \Vert^2) \\ F(w_{T+1}) - F(w_*) &\leq \frac{L}{2T}(\Vert w_1 - w_* \Vert^2) \end{align*}$$ where we also use the property that $F(w_t)$ is non-increasing in the second step.

---

Moreover, if we also assume strong convexity with $\mu$, we converge even faster, with $$\Vert w_{t+1} - w_* \Vert_2^2 \leq \left(1 - \frac{\mu}{L}\right)^\top \Vert w_1 - w_* \Vert_2^2$$ and convergence rate $O(\log \frac{1}{\epsilon})$.