Policy gradients is a class of reinforcement learning algorithms that directly optimize our objective, $$\theta^* = \arg\max_\theta J(\theta)$$ where $$J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ \sum_{t=1}^T r(s_t, a_t) \right] = \mathbb{E}_{\rho(s)}[V^\pi(s)] = \mathbb{E}_{\rho(s, a)}[Q^\pi(s, a)].$$

We seek to find the gradient of our objective with respect to $\theta$ and use [[‚õ∞Ô∏è Gradient Descent]], $$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta).$$ Unfortunately, directly calculating $\nabla_\theta J(\theta)$ is impossible since the trajectories depend on the environment's dynamics. However, we can derive another form, known as the policy gradient theorem: $$\nabla_\theta J(\theta) \propto \sum_s \rho(s) \sum_a Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s).$$ To actually calculate this value, we can further rewrite it with the [[ü¶Ñ Log Derivative Trick]], so $$\nabla_\theta J(\theta) \propto \mathbb{E}_{s, a \sim \rho(s, a)}[Q^\pi(s, a) \nabla_\theta \log \pi_\theta(a \vert s)].$$

In the proof for this theorem, the constant of proportionality is the average length of an episode for the episodic case and equal to $1$ for the infinite horizon case. We can also arrive at an equality by "capturing" this constant in the expectation by calculating over full trajectories $\tau$, $$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ \left( \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t \vert s_t) \right)\left( \sum_{t=1}^T r(s_t, a_t) \right) \right].$$ Intuitively, this is a supervised learning update with the gradients scaled by the reward; thus if the reward is high, then our policy is more likely to perform this trajectory again. In a sense, this is simply "trial and error."

The above formula is the core equation for many policy gradient algorithms; more generally, most can be expressed in the form $$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ \sum_{t=0}^T \Psi_t \nabla_\theta \log \pi_\theta (a_t \vert s_t) \right]$$ where $\Psi_t$ can be one of multiple choices: $$\Psi_t = \begin{cases} \sum_{t'=0}^T r_{t'} & \text{total trajectory reward} \\ \sum_{t'=t}^T r_{t'} & \text{reward after $a_t$ (return)} \\ \sum_{t'=t}^T r_{t'} - b(s_t) \text{ for some baseline $b(s_t)$} & \text{baselined return} \\ Q^\pi(s_t, a_t) & \text{action-value function} \\ A^\pi(s_t, a_t) & \text{advantage function} \\ r_t + V^\pi(s_{t+1}) - V^\pi(s_t) & \text{TD residual} \end{cases}$$