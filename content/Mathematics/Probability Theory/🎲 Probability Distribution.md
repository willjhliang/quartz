Probability space is defined by sample space $\Omega$, event space $\mathcal{A}$, and probabilities $P$.
1. The sample space is the set of all possible outcomes.
2. The event space is the set of possible results, each result being a set of outcomes $\omega$ from the sample space.
3. Probabilities are associated with each event $A \in \mathcal{A}$ to measure how likely it is for $A$ to occur.

Probability distributions use probability space to describe how likely a [[🪐 Random Variable]] is to take on each of its different states. These states belong in the target space $\mathcal{T}$.

For random variable $X$ and subset $S \subseteq \mathcal{T}$, we can associate a probability $P_X(S)$ based on the probability of the particular event corresponding to $X$. Mathematically, 
$$
P_X(S) = P(X \in S) = P(\{\omega \in \Omega \mid X(\omega) \in S\}).
$$


These probabilities can be either discrete or continuous.
1. If $\mathcal{T}$ is discrete, we can specify $P(X = x)$ for $x \in \mathcal{T}$ using the probability mass function.
2. If $\mathcal{T}$ is continuous, we instead specify $P(X \leq x)$ using the cumulative distribution function.

# Types of Distributions
Distributions with random variables $X$ and $Y$ can have multiple types.
1. Joint probability models probabilities for each pair $p(x, y)$.
2. Marginal probability for $X$ finds probabilities for $X = x$ irrespective of what value is taken for $Y$. This is defined as $p(x)$.
3. Conditional probability considers only instances for $X = x$ to find probabilities for $Y = y$ and is written as $p(y \vert x)$.

# Discrete Probabilities
With a discrete target space, each outcome has its own probability, so our probability mass function 
$$
P(X = x) = \frac{n}{N}
$$
 where $n$ is the number of events with state $x$ and $N$ is the total number of events.

# Continuous Probabilities
In continuous target space, we need functions to define probabilities rather than explicit fractions. The probability density function $f$ is non-negative and satisfies 
$$
\int_{\mathbb{R}^D} f(x)dx = 1.
$$
 Using $f$, we can find 
$$
P(X \leq x) = \int_{-\infty}^x f(x)dx.
$$


This value is captured by the cumulative distribution function, 
$$
F_x(x) = P(X \leq x).
$$


# Probability Rules
Given a joint distribution $p(x, y)$, we can use the sum rule to find the marginal: 
$$
p(x) = \begin{cases} \sum_y p(x, y) & \text{if $y$ is discrete} \\ \int_y p(x, y)dy & \text{if $y$ is continuous} \end{cases}
$$

> [!info]
> Many computational challenges in probabilistic modeling come from the sum rule. If we have many random variables, it's computationally expensive to calculate the sum or integral over them.

We can also relate the joint to the conditional and marginal using the product rule, 
$$
p(x, y) = p(y \vert x)p(x) = p(x \vert y)p(y).
$$
 Using this property, we can derive [[🪙 Bayes' Theorem]], 
$$
p(x \vert y) = \frac{p(y \vert x)p(x)}{p(y)}.
$$

